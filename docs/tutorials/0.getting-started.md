# Getting Started with StackBench

Welcome to StackBench! This tutorial will walk you through your first benchmark run, from installation to analyzing results.

## What is StackBench?

StackBench is a local tool that benchmarks how well coding agents (like Cursor) perform on library-specific tasks. It tests whether AI assistants can correctly use APIs, follow patterns, and implement realistic use cases from your documentation.

**Key Benefits:**
- üè† **Local execution** - Your code never leaves your machine
- üìö **Library-focused** - Tests real-world usage patterns
- ‚ö° **IDE integration** - Works with your existing development workflow
- üéØ **Actionable insights** - Identify documentation gaps and common failures

## Prerequisites

Before we start, ensure you have:

### System Requirements
- **Python 3.10+** (check with `python --version`)
- **Node.js 18+** (check with `node --version`)
- **Git** (check with `git --version`)

### Required Tools

**1. Install uv (Python package manager):**
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**2. Install Claude Code CLI:**
```bash
npm install -g @anthropic-ai/claude-code
```

### API Keys

You'll need API keys for:
- **OpenAI** - For extracting use cases from documentation
- **Anthropic** - For analyzing coding agent performance

Get your keys from:
- OpenAI: https://platform.openai.com/api-keys
- Anthropic: https://console.anthropic.com/

## Installation

### 1. Clone and Install StackBench

```bash
# Clone the repository
git clone https://github.com/your-org/stackbench
cd stackbench

# Install dependencies
uv sync

# Verify installation
uv run stackbench --version
```

### 2. Configure Environment

```bash
# Copy the sample environment file
cp .env.sample .env

# Edit .env and add your API keys:
# OPENAI_API_KEY=your_openai_key_here
# ANTHROPIC_API_KEY=your_anthropic_key_here
```

**Important:** Never commit your API keys to git. The `.env` file is already in `.gitignore`.

## Your First Benchmark

Let's benchmark how well Cursor handles a real library. We'll use the DSPy library as an example.

### Step 1: Set Up the Benchmark

```bash
# Set up DSPy library benchmark with Cursor agent (Python library)
uv run stackbench setup https://github.com/stanfordnlp/dspy -a cursor -i docs -l python
```

This command:
- Clones the DSPy repository
- Creates a unique benchmark run with UUID
- Extracts use cases from the `docs/` folder
- Prepares everything for manual execution

**Expected output:**
```
üöÄ Setting up new benchmark run...
üìÅ Cloned repository to: ./data/abc123.../repo/
üîç Extracting use cases from documentation...
‚úÖ Generated 5 use cases

üìã Next Steps:
1. Run: stackbench print-prompt abc123... -u 1 -c
2. Open Cursor in: ./data/abc123.../repo/
3. Paste prompt and implement use case
4. Repeat for all 5 use cases
5. Run: stackbench analyze abc123...

Run ID: abc123...
```

### Step 2: Execute Use Cases in Cursor

Now for the fun part - let's see how Cursor handles library-specific tasks!

**For each use case:**

1. **Get the formatted prompt:**
   ```bash
   uv run stackbench print-prompt abc123... -u 1 --copy
   ```
   
   This copies a detailed prompt to your clipboard with:
   - Use case description and requirements
   - Target audience and complexity level  
   - Specific implementation requirements
   - Where to save your solution

2. **Open Cursor IDE:**

3. **‚ö†Ô∏è Wait for Cursor indexing to complete** - Check Settings ‚Üí "Indexing & Docs" ‚Üí "Codebase Indexing" and wait until it shows 100%. This is critical for Cursor to understand the library's APIs and patterns.

4. **Start a new chat session** and paste the prompt

5. **Let Cursor explore** the repository and propose a solution

6. **Review and accept** the implementation (or make modifications)

7. **Repeat for all use cases** (use `-u 2`, `-u 3`, etc.)

### Step 3: Analyze Results

Once you've implemented all use cases:

```bash
uv run stackbench analyze abc123...
```

This will:
- Test each implementation for correctness
- Analyze library usage patterns  
- Evaluate documentation consultation
- Generate a comprehensive report

**Sample analysis output:**
```
üîç Analyzing 5 use cases...
‚ö° Testing use case 1... ‚úÖ Executable
‚ö° Testing use case 2... ‚ùå Import error
‚ö° Testing use case 3... ‚úÖ Executable  
‚ö° Testing use case 4... ‚ö†Ô∏è  Partial (mocked)
‚ö° Testing use case 5... ‚úÖ Executable

üìä Results saved to:
- ./data/abc123.../results.json
- ./data/abc123.../results.md
```

### Step 4: Review Results

Open the generated report:

```bash
# View structured results
cat ./data/abc123.../results.json

# Read human-friendly analysis
cat ./data/abc123.../results.md
```

The report will show:
- **Pass/Fail Status:** Did Cursor successfully handle DSPy tasks?
- **Success Rate:** 4/5 tasks successful (80%)
- **Common Failures:** API deprecation issues, missing imports
- **Insights:** Specific improvements needed in documentation

## Understanding the Results

### Success Indicators ‚úÖ
- Code executes without errors
- Uses real library APIs (not mocked)
- Follows library conventions
- Implements functional requirements

### Warning Signs ‚ö†Ô∏è  
- Code uses mocking instead of real APIs
- Missing error handling
- Outdated API usage
- Partial implementation

### Failure Patterns ‚ùå
- Syntax errors or import failures
- Deprecated API usage
- Incorrect configuration
- Missing dependencies

## Common Issues and Solutions

### "Module not found" errors
**Problem:** Cursor tries to import non-existent modules
**Solution:** Check if your documentation references outdated imports

### High mocking rate  
**Problem:** Cursor creates fake implementations instead of using real APIs
**Solution:** Your documentation may lack clear, executable examples

### Low success rate
**Problem:** Most use cases fail to execute
**Solution:** Consider updating installation instructions or API examples

## Next Steps

Now that you've completed your first benchmark:

1. **Review the specific failures** in your results report
2. **Update your documentation** based on the insights  
3. **Re-run the benchmark** to validate improvements
4. **Try different libraries** to expand your understanding
5. **Share results** with your team or library maintainers

## Command Quick Reference

```bash
# Set up new benchmark
stackbench setup <repo-url> -a cursor -i <folders>

# Get use case prompt
stackbench print-prompt <run-id> -u <number> --copy

# Analyze implementations  
stackbench analyze <run-id>

# Check status
stackbench status <run-id>

# List all runs
stackbench list
```

## What's Next?

- **[Cursor Workflow Guide](1.cursor-workflow.md)** - Advanced IDE integration techniques
- **[How StackBench Works](2.how-stackbench-works.md)** - Understanding the pipeline stages
- **[CLI Commands Reference](3.cli-commands.md)** - Complete command documentation

## Getting Help

- **Check status:** `stackbench status <run-id>` for detailed progress
- **View logs:** Look in `./data/<run-id>/` for execution details
- **Community:** Open issues on GitHub for support

---

**Congratulations!** You've successfully completed your first StackBench benchmark. You now have concrete insights into how well coding agents handle your library's specific use cases.